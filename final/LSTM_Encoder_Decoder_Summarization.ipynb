{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhcnzSm4yS5s"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow tensorflow-datasets transformers\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load the Xsum dataset from Huggingface\n",
        "dataset = tfds.load('xsum', split='train', shuffle_files=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the Huggingface tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/pegasus-xsum')\n",
        "\n",
        "# Define a function to preprocess the input and output text\n",
        "def preprocess_data(example):\n",
        "    input_text = example['document'].numpy().decode('utf-8')\n",
        "    output_text = example['summary'].numpy().decode('utf-8')\n",
        "    inputs = tokenizer.encode_plus(input_text, max_length=512, truncation=True, padding='max_length', return_tensors='tf')\n",
        "    outputs = tokenizer.encode_plus(output_text, max_length=128, truncation=True, padding='max_length', return_tensors='tf')\n",
        "    return inputs, outputs\n",
        "\n",
        "# Preprocess the dataset\n",
        "dataset = dataset.map(preprocess_data)"
      ],
      "metadata": {
        "id": "iPc62j_xyWQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "BJWAW2KiyY9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the input and output sequence lengths\n",
        "max_input_length = 512\n",
        "max_output_length = 128\n",
        "\n",
        "# Define the LSTM encoder-decoder model\n",
        "encoder_inputs = Input(shape=(max_input_length,), name='encoder_inputs')\n",
        "encoder_embedding = Embedding(input_dim=tokenizer.vocab_size, output_dim=128, name='encoder_embedding')(encoder_inputs)\n",
        "encoder_lstm1 = LSTM(256, return_sequences=True, name='encoder_lstm1')(encoder_embedding)\n",
        "encoder_lstm2 = LSTM(256, return_sequences=True, name='encoder_lstm2')(encoder_lstm1)\n",
        "encoder_lstm3 = LSTM(256, name='encoder_lstm3')(encoder_lstm2)\n",
        "\n",
        "decoder_inputs = Input(shape=(max_output_length,), name='decoder_inputs')\n",
        "decoder_embedding = Embedding(input_dim=tokenizer.vocab_size, output_dim=128, name='decoder_embedding')(decoder_inputs)\n",
        "decoder_lstm1 = LSTM(256, return_sequences=True, name='decoder_lstm1')(decoder_embedding, initial_state=[encoder_lstm3, encoder_lstm3])\n",
        "decoder_lstm2 = LSTM(256, return_sequences=True, name='decoder_lstm2')(decoder_lstm1, initial_state=[encoder_lstm3, encoder_lstm3])\n",
        "decoder_lstm3 = LSTM(256, return_sequences=True, name='decoder_lstm3')(decoder_lstm2, initial_state=[encoder_lstm3, encoder_lstm3])\n",
        "decoder_outputs = Dense(tokenizer.vocab_size, activation='softmax', name='decoder_outputs')(decoder_lstm3)\n",
        "\n",
        "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n"
      ],
      "metadata": {
        "id": "aCPrOxT_yblz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Define the callbacks\n",
        "checkpoint_path = 'best_model.h5'\n",
        "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=False, mode='min', verbose=1)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, mode='min', verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, mode='min', verbose=1)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=[checkpoint, early_stop, reduce_lr])"
      ],
      "metadata": {
        "id": "Ck202RytzRt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we define the callbacks to save the best model using ModelCheckpoint, stop the training early if the validation loss does not improve for 5 epochs using EarlyStopping, and reduce the learning rate by a factor of 0.1 if the validation loss does not improve for 2 epochs using ReduceLROnPlateau. We then compile the model with the Adam optimizer and sparse categorical crossentropy loss, and fit the model to the training and validation datasets for 20 epochs with the defined callbacks. The history object contains the training and validation metrics for each epoch."
      ],
      "metadata": {
        "id": "RjPt69Iuzaod"
      }
    }
  ]
}